stage: dense_gpu_baseline
seq_len: 2048

tasks: [boolq, hellaswag, gsm8k]

# IMPORTANT: set these to the exact checkpoints you want (Phoenix paper may use specific ones)
models:
  llama31_8b:
    model_id: "meta-llama/Meta-Llama-3.1-8B"
    dtype: "bf16"
    per_device_train_bs: 1
    grad_accum: 8
  mistral_7b:
    model_id: "mistralai/Mistral-7B-v0.1"
    dtype: "bf16"
    per_device_train_bs: 1
    grad_accum: 8
  mixtral_8x7b:
    model_id: "mistralai/Mixtral-8x7B-v0.1"
    dtype: "bf16"
    per_device_train_bs: 1
    grad_accum: 16   # allow override; MoE sometimes needs higher GA

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

train:
  lr: 2.0e-4
  warmup_ratio: 0.03
  weight_decay: 0.0
  num_epochs: 1
  max_steps: 100
  logging_steps: 10
  eval_steps: 200
  save_steps: 200
  gradient_checkpointing: true
  packing: true
  completion_only_loss: true

throughput_sweep:
  batch_sizes: [64, 128, 256]
  warmup_steps: 3
  timed_steps: 1
